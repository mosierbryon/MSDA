---
title: "Group 10 Project, Summer 2018"
author: "Bryon Mosier, Hejin Shin, Veronica Stephens"
date: "August 7, 2018"
output:
  word_document:
    highlight: zenburn
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# import libraries
library(dplyr)
library(ggplot2)
library(caret)
library(lubridate)
library(pls)
library(doParallel)
library(AppliedPredictiveModeling) 
library(lattice)
library(corrplot) 
library(elasticnet)
library(reshape2)  
library(RANN)
library(Hmisc)
library(forecast)
library("TTR", lib.loc="C:/Program Files/R/R-3.4.3/library")
library(Ecdat)


```

```{r}
# after creating final repository, fix this chunk

# define base github url
# git_base <- c("https://raw.githubusercontent.com/veronicastephens/...")
# read data
# fname <- c('mean_unemployment_length')
# df_length <- read.csv(paste(git_base,fname,sep='/'))

```


```{r}
# read csv: unemployment length 
df_length <- read.csv('https://raw.githubusercontent.com/mosierbryon/MSDA/master/mean_unemployment_length.csv')
names(df_length) <- c('date','mean_unemp_weeks')
df_length <- df_length %>% mutate(date=as.Date(date,format='%m/%d/%Y'))

# scatter plot: date and mean weekly unemployment
p <- ggplot(df_length,aes(x=date,y=mean_unemp_weeks)) + geom_point(stat='identity') + theme_classic()
p

# read csv: unemployment rate
df_rate <- read.csv('https://raw.githubusercontent.com/mosierbryon/MSDA/master/Unemployment.csv')
df_rate <- df_rate %>% mutate(Month=as.character(Month),
                               date=parse_date_time2(paste(Year,Month,'01',sep='-'),'%Y-%m-%d')) %>%  
                       select(date,Unemployment_Rate,rate_women) %>% 
                       mutate(date=as.Date(date))

# combine dfs
df_rate <- inner_join(df_length,df_rate,by=c('date'='date'))  
df_rate <- df_rate %>% select(date,everything())

# read csv: fred data
df_fred <- read.csv('https://raw.githubusercontent.com/mosierbryon/MSDA/master/fred_mo_1919_2018_180715.csv')
names(df_fred)[1] <- c('date')
df_fred <- df_fred %>% mutate(date=as.Date(date,format='%Y-%m-%d'), year=year(date))


df <- left_join(df_fred,df_rate,by=c('date','date'))
df <- df %>% select(date,year,everything()) %>% filter(date >= '1948-01-01', date<'2018-06-01') %>% 
              select(-PSAVERT) %>% mutate(month=month(date))
df <- df %>% select(-month) %>% select(-year)

# define number of predictors
nvar <- ncol(df)-1

# response var: df$mean_uemp_weeks
# pred vars: date, INDPRO,CIVPART,CPIAUCSL,PAYEMS,Uemployment_Rate,rate_women

```


```{r}
# decomposing time series

p <- ggplot(df, aes(x=date,y=mean_unemp_weeks)) + geom_line() + theme_classic()
p

ts_unemp = ts(df)
plot((ts_unemp))


# https://anomaly.io/seasonal-trend-decomposition-in-r/
# plots
df <- df %>% filter(date < '2018-01-01')
df[840,]
ts_unemp = ts(df[,6])
plot((ts_unemp))

# # Detect the Trend
# # ma = moving-average smoothing
# trend_unemp = ma(df$mean_unemp_weeks, order = 12, centre = T)
# plot(as.ts(ts_unemp))
# lines(trend_unemp)
# plot(as.ts(trend_unemp))
# 
# # Detrend the Time Series
# # Removing the previously calculated trend from the time series will result into a new time series that clearly exposes seasonality.
# detrend_unemp = ts_unemp / trend_unemp
# plot(as.ts(detrend_unemp))
# 
# 
# # Average the Seasonality
# m_air = t(matrix(data = detrend_unemp, nrow = 12)) #append june-dec 2018 NA to df to use nrow=12
# seasonal_unemp = colMeans(m_air, na.rm = T)
# plot(as.ts(rep(seasonal_unemp,12))) #change to 12 after changing nrow
# 
# # Examining Remaining Random Noise
# # multiplicative formula is "Time series = Seasonal * Trend * Random", which means "Random = Time series / (Trend * Seasonal)"
# random_unemp = ts_unemp / (trend_unemp * seasonal_unemp)
# plot(as.ts(random_unemp))
# 
# # Reconstruct the Original Signal
# recomposed_unemp = trend_unemp*seasonal_unemp*random_unemp
# plot(as.ts(recomposed_unemp))


# ---------------------------
ts_unemp = ts(ts_unemp, frequency = 12)
decompose_unemp = decompose(ts_unemp, "multiplicative")
 
plot(as.ts(decompose_unemp$seasonal))
plot(as.ts(decompose_unemp$trend))
plot(as.ts(decompose_unemp$random))
plot(decompose_unemp)

# # additive
# ts_beer = ts(ts_unemp, frequency = 4)
# decompose_beer = decompose(ts_beer, "additive")
#  
# plot(as.ts(decompose_beer$seasonal))
# plot(as.ts(decompose_beer$trend))
# plot(as.ts(decompose_beer$random))
# plot(decompose_beer)


```



```{r}
# Holt Winters Forecast
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
# Read 100 items
rainseries <- ts(rain,start=c(1813))
plot.ts(rainseries)
rainseriesforecasts <- HoltWinters(rainseries, beta=FALSE, gamma=FALSE)
rainseriesforecasts2 <- forecast(rainseriesforecasts, h=8)
rainseriesforecasts2
plot(rainseriesforecasts2)
acf(rainseriesforecasts2$residuals, lag.max=10)




```

 
```{r}


eeadj <- seasadj(stl(df$mean_unemp_weeks, s.window="periodic"))
plot(eeadj)
tsdisplay(diff(eeadj),main="")




# difference plot
df2 <- df %>% select(date,mean_unemp_weeks)
df2 <- as.ts(df2)

# acf, pacf
tsdisplay(diff(df2),main="")

mean_unemp_weeks <- df$mean_unemp_weeks
plot(diff(mean_unemp_weeks))

```

```{r}
# models in franks presentation: ar, ma, 
?timeslice
```



```{r}
# data preprocessing

# examing missing data in columns
df_col = data.frame(sapply(df, function(x) sum(is.na(x))))
names(df_col) <- c('missing')
df_col$names <- rownames(df_col)
df_col <- df_col %>% arrange(-missing) %>% select(names,missing)
df_col[1:ncol(df),]

# examine missing data in rows
df_rows = data.frame(apply(df, 1, function(x) sum(is.na(x))))
names(df_rows) <- c('missing')
df_rows$names <- rownames(df_rows)
df_rows <- df_rows %>% select(names,missing) %>% arrange(-missing)
df_rows[1:10,]


# correlation plot





```



```{r}
# initial models:

# linear model
# lm_model = lm(df$mean_unemp_weeks~.,data = df)
# summary(lm_model)

# trainControl: control the computational nuances of the train function {caret}
myTimeControl <- trainControl(method = "timeslice",
                              initialWindow = 36,
                              horizon = 12,
                              fixedWindow = TRUE)

tuneLength.num <- 5

# fractionTune = c(seq(.005, .03,.005),seq(.04,.1,0.02),seq(0.2,1,0.2))
# lambdaTune = c(0, .001, .01, .1, 1)
# 
# set.seed(127)
# model_enet <- train(x = train_pred, y = train_resp, 
#                   method = "enet", 
#                   trControl = train_ctrl, 
#                   preProcess = c("center", "scale"), 
#                   tuneGrid = expand.grid(lambda = lambdaTune,
#                                          fraction = fractionTune))



glmnet.mod <- train(mean_unemp_weeks ~ .-date,
                    data = df,
                    method = "glmnet",
                    family = "gaussian",
                    trControl = myTimeControl,
                    tuneLength=tuneLength.num)

pois.mod <- train(mean_unemp_weeks ~ .,
                  data = df,
                  method = "glmnet",
                  family = "poisson",
                  trControl = myTimeControl,
                  tuneLength=tuneLength.num)

lm.mod <- train(mean_unemp_weeks ~ .,
                data = df,
                method = "lm",
                trControl = myTimeControl,
                tuneLength=tuneLength.num)

earth.mod <- train(mean_unemp_weeks ~ .,
                   data = df,
                   method = "earth",
                   trControl = myTimeControl,
                   tuneLength=tuneLength.num)


earth.pois.mod <- train(mean_unemp_weeks ~ .,
                        data = df,
                        method = "earth",
                        glm=list(family=poisson),
                        trControl = myTimeControl,
                        tuneLength=tuneLength.num)

gam.mod <- train(mean_unemp_weeks ~ .,
                 data = df,
                 method = "gam",
                 trControl = myTimeControl,
                 tuneLength=tuneLength.num)

rpart.mod <- train(mean_unemp_weeks ~ .,
                   data = df,
                   method = "rpart",
                   trControl = myTimeControl,
                   tuneLength=tuneLength.num)

rpart.mod <- train(mean_unemp_weeks ~ .,
                   data = df,
                   method = "rpart",
                   trControl = myTimeControl,
                   tuneLength=tuneLength.num)

rf.mod <- train(mean_unemp_weeks ~ .,
                data = df,
                method = "rf",
                trControl = myTimeControl,
                tuneLength=tuneLength.num)

gbm.mod <- train(mean_unemp_weeks ~ .,
                 data = df,
                 method = "gbm",
                 distribution="poisson",
                 trControl = myTimeControl,
                 tuneLength=tuneLength.num,
                 verbose=FALSE)

### model performance ###
resamps <- resamples(list(glmnet = glmnet.mod,
                          glmnet.pois = pois.mod,
                          lm = lm.mod,
                          earth=earth.mod,
                          earth.pois=earth.pois.mod,
                          gbm=gbm.mod,
                          gam=gam.mod,
                          rf=rf.mod,
                          rpart=rpart.mod,
                          party=party.mod))
resamps

ss <- summary(resamps)

knitr::kable(ss[[3]]$Rsquared)

library(lattice)

trellis.par.set(caretTheme())
dotplot(resamps, metric = "Rsquared")library(earth)

library(earth)
plotmo(earth.mod$finalModel)





# method = 'lm' #linear regression, intercept tuning parameter
# method = 'lmStepAIC' #Linear Regression with Stepwise Selection, no tuning parameters, library(MASS)
# method = 'pls' #Partial Least Squares, tuning parameter: ncomp=number of components
# # A model-specific variable importance metric is available.
# method = 'ridge' #ridge regression, tuning parameter= lambda, weight decay, library(elasticnet)



# pls model
plsFitTime <- train(mean_unemp_weeks ~ date + Unemployment_Rate + rate_women +
                      INDPRO + CIVPART + CPIAUCSL + PAYEMS,
                    data = df,
                    method = "pls", 
                    preProc = c("center", "scale"),
                    trControl = myTimeControl)
plsFitTime 
# Rsquared values:
# 47.18%, pred = date + year
# 41.71%, pred = date + Unemployment_Rate
# 40.62%, pred = date + Unemployment_Rate + rate_women




```


```{r}
# using tsCV function from forecast package
# 







```



```{r}
# for reference

# library(caret)
# data(GermanCredit)
# set.seed(1056)
# svmFit <- train(Class ~ .,
#                   data = GermanCreditTrain,
#                   # The "method" argument indicates the
#                   # model type.
#                   # See ?train for a list of available models.
#                   method = "svmRadial")


## source: https://stackoverflow.com/questions/24758218/time-series-data-spliting-and-model-evaluation?lq=1
# data(economics)
# df <- economics
# 
# # trainControl: control the computational nuances of the train function {caret}
# myTimeControl <- trainControl(method = "timeslice",
#                               initialWindow = 36,
#                               horizon = 12,
#                               fixedWindow = TRUE)
# # original: timeSlices <- createTimeSlices(1:nrow(df),initialWindow = 36, horizon = 12, fixedWindow = TRUE)
# 
# plsFitTime <- train(unemploy ~ pce + pop + psavert,
#                     data = economics,
#                     method = "pls",
#                     preProc = c("center", "scale"),
#                     trControl = myTimeControl)
# plsFitTime

```
















